{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Model will be automatically downloaded from HuggingFace model hub if not cached.\n",
    "# Model files will be cached in \"~/.cache/huggingface/hub/models--NECOUDBFM--Jellyfish/\" by default.\n",
    "# You can also download the model manually and replace the model name with the path to the model files.\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"NECOUDBFM/Jellyfish\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NECOUDBFM/Jellyfish\")\n",
    "\n",
    "system_message = \"You are an AI assistant that follows instruction extremely well. Help as much as you can.\"\n",
    "\n",
    "# You need to define the user_message variable based on the task and the data you want to test on.\n",
    "user_message = \"\"\"You are tasked with determining whether two records listed below are the same based on the information provided.\n",
    "Carefully compare the {attribute 1}, {attribute 2}... for each record before making your decision.  \n",
    "Note: Missing values (N/A or \\\"nan\\\") should not be used as a basis for your decision.  \n",
    "Record A: [Company: CreativeWorks, Location: Seattle, WA, Industry: Design, Description: Creative agency offering graphic design and branding services.]\n",
    "Record B: [Name: Chris Wilson, Education: B.F.A. in Graphic Design, RISD, Position: Senior Graphic Designer, Skills: ['Adobe Creative Suite', 'Branding'], Role: Graphic Designer]\n",
    "Are record A and record B the same entity? Choose your answer from: [Yes, No].\"\"\"\n",
    "\n",
    "prompt = f\"{system_message}\\n\\n### Instruction:\\n\\n{user_message}\\n\\n### Response:\\n\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "# You can modify the sampling parameters according to your needs.\n",
    "generation_config = GenerationConfig(\n",
    "    do_samples=True,\n",
    "    temperature=0.35,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=1024,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.15,\n",
    "    )\n",
    "\n",
    "output = generation_output[0]\n",
    "response = tokenizer.decode(\n",
    "    output[:, input_ids.shape[-1] :][0], skip_special_tokens=True\n",
    ").strip()\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
