{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfe5667443b419b85bc00f75476ac2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded in 67.59 seconds.\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "import torch\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# Specify an offload folder for weights offloaded to disk\n",
    "offload_folder = \"./offload_weights\"\n",
    "\n",
    "# Start timing the loading process\n",
    "start_time = time.time()\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"NECOUDBFM/Jellyfish\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=offload_folder,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NECOUDBFM/Jellyfish\")\n",
    "\n",
    "# End timing the loading process\n",
    "end_time = time.time()\n",
    "print(f\"Model and tokenizer loaded in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "system_message = \"You are an AI assistant that follows instruction extremely well. Help as much as you can.\"\n",
    "\n",
    "# Define the user_message variable\n",
    "user_message = \"\"\"You are tasked with determining whether two records listed below are the same based on the information provided.\n",
    "Carefully compare the Company, Location, Industry, Description, Name, Education, Position, Skills for each record before making your decision.\n",
    "Note: Missing values (N/A or \\\"nan\\\") should not be used as a basis for your decision.\n",
    "Record A: [Company: CreativeWorks, Location: Seattle, WA, Industry: Design, Description: Creative agency offering graphic design and branding services.]\n",
    "Record B: [Name: Chris Wilson, Education: B.F.A. in Graphic Design, RISD, Position: Senior Graphic Designer, Skills: ['Adobe Creative Suite', 'Branding'], Role: Graphic Designer, Company: CreativeWorks]\n",
    "Are record A and record B the same entity? Choose your answer from: [Yes, No].\"\"\"\n",
    "\n",
    "prompt = f\"{system_message}\\n\\n### Instruction:\\n\\n{user_message}\\n\\n### Response:\\n\\n\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "# Define the generation configuration\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    temperature=0.35,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Generate the response\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=1024,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        repetition_penalty=1.15,\n",
    "    )\n",
    "\n",
    "output = generation_output.sequences\n",
    "response = tokenizer.decode(\n",
    "    output[:, input_ids.shape[-1]:][0], skip_special_tokens=True\n",
    ").strip()\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context-match",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
